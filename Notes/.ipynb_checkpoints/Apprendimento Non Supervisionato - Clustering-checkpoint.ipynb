{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sistemi di Apprendimento Non Supervisionato**\n",
    "\n",
    "## **Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il **clustering** è l'operazione di **categorizzare** dei campioni in un dataset senza che questi abbiano necessariamente un'*etichetta* determinata a priori (*non c'è una label*).\n",
    "\n",
    "Per fare un esempio, potremmo suddividere i nostri album musicali sulla base delle sonorità ispirate dal loro ascolto: in questo caso, non ci staremmo affidando ad una certa \"etichetta\", come ad esempio l'anno di produzione o l'artista, ma ad un concetto molto più \"empirico\", ovvero la vicinanza o meno dell'album ai nostri gusti musicali.\n",
    "\n",
    "Ovviamente, dato che nel clustering i **campioni non considerano una label**, stiamo parlando di **apprendimento non supervisionato**. Se i campioni fossero etichettati, avremmo una normale procedura di classificazione.\n",
    "\n",
    "Il clustering può avere numerose applicazioni: ad esempio, potrebbe essere usato per segmentare il mercato mediante dei profili di clientela simili, oppure per suddividere le immagini in zone simili, o ancora per individuare delle anomalie all'interno di un insieme di dati.\n",
    "\n",
    "Una volta che il clustering è completo, ad ogni **cluster** viene assegnato un certo *identificativo*, che ci permette in qualche modo di \"condensare\" e \"riassumere\" le informazioni dell'intero cluster.\n",
    "\n",
    "Quest'assegnazione può anche essere usata *come ingresso* ad altri sistemi di machine learning, ad esempio di classificazione, che possono usare l'identificativo assegnato come una vera e propria label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tipi di Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La scelta di un algoritmo di clustering deve essere condotta sulla base della **scalabilità** dello stesso. \n",
    "\n",
    "Infatti, laddove alcuni algoritmi di clustering confrontano tra loro *ogni possibile coppia di dati*, con una complessità $O(n^2)$ per $n$ campioni, altri, come il **k-means**, effettuano un numero molto più limitato di operazioni, ottenendo una complessità nell'ordine di $O(n)$ , il che cambia radicalmente la situazione nel caso di dataset con milioni di campioni.\n",
    "\n",
    "Tuttavia, ogni algoritmo ha anche diversi vantaggi e svantaggi che devono essere valutati sulla base dell'applicazione scelta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In genere ci sono quattro diverse categorie di clustering:\n",
    "\n",
    "- nel **centroid-based clustering**, i dati sono organizzati secondo la loro *distanza* da dei *centroidi*, ovvero dei campioni considerati come \"base\" per ciascun cluster. Questo tipo di algoritmi risulta essere mediamente efficace, ma è sensibile alle condizioni iniziali ed alla presenza di eventuali outliers;\n",
    "\n",
    "- nel **density-based clustering**, i dati sono organizzati in *aree ad alta densità*. Ciò permette la connessione di cluster di forma arbitraria, e facilita inoltre l'individuazione di outlier, che per definizione sono nelle zone a minore densità di campioni. Possono però essere sensibili a dataset con densità variabile ed alta dimensionalità;\n",
    "\n",
    "- nel **distribution-based clustering**, si suppone che i dati abbiano *distribuzione gaussiana*, e siano quindi suddivisibili come tali. Questo tipo di algoritmi non è efficiente se non si conosce a priori il tipo di distribuzione dei dati;\n",
    "\n",
    "- nello **hierarchical clustering** viene creato un *albero* a partire dai dati. Questo tipo di clustering è particolarmente efficace nel caso si trattino certi tipi di dati, come ad esempio le tassonomie, e prevede che possa essere selezionato un numero ridotto di cluster tagliando l'albero al giusto livello.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Workflow del clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'esecuzione di un algoritmo di clustering prevede *tre step*:\n",
    "\n",
    "1. nel primo, dobbiamo *preparare i dati*, effettuando le operazioni che abbiamo visto in precedenza per la classificazione e la regressione;\n",
    "\n",
    "2. nel secondo, dovremo definire una *metrica di similarità*;\n",
    "\n",
    "3. nel terzo, eseguiremo l'*algoritmo* vero e proprio.\n",
    "\n",
    "Concentriamoci per un attimo sul secondo step. Definire una **metrica di similarità** significa nella pratica stabilire *quando due campioni risultano essere simili tra loro*.\n",
    "\n",
    "In tal senso, è possibile operare in due modi:\n",
    "\n",
    "- la metrica può essere scelta *manualmente*, ovvero scegliendo le feature da considerare nella valutazione della distanza tra i campioni;\n",
    "\n",
    "- oppure, la metrica può essere scelta in maniera *automatica* a partire da un **embedding**, ovvero da una *rappresentazione a dimensionalità ridotta* del dato iniziale.\n",
    "\n",
    "Nel primo caso questo avviene in modo abbastanza intuitivo: se, ad esempio, volessimo suddividere un insieme di scarpe in base a taglia e prezzo, potremmo considerare la distanza euclidea come rappresentativa dello \"spazio\" che intercorre tra due campioni. Questo approccio, tuttavia, è efficace soltanto nel caso di campioni a bassa dimensionalità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](Images/workflow_cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il secondo caso è invece preferibile nel momento in cui si vanno a considerare dei dati ad alta dimensionalità: infatti, in queste situazioni si rischia di incorrere nel fenomeno della *curse of dimensionality*, che rende difficile distinguere tra due campioni differenti, per cui si tende ad estrarre delle rappresentazioni \"ridotte\" dei dati a partire dalle quali applicare il concetto di distanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embadding](Images\\embadding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **K-Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo adesso come usare il più conosciuto ed utilizzato algoritmo di clustering, ovvero il **k-means**, algoritmo **centroid-based** che raggruppa i campioni in $k$ diversi cluster assegnando ogni dato in base alla distanza dal *centroide* del cluster stesso.\n",
    "\n",
    "Il k-means ha diverse ipotesi alla base, tra cui la più restrittiva è una, ovvero quella legata alla *conoscenza del numero iniziale di cluster $k$*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta fissato questo valore, l'algoritmo lavora in tre step successivi:\n",
    "\n",
    "1. al primo step, l'*algoritmo sceglie casualmente $k$ centroidi* tra i diversi dati a disposizione;\n",
    "\n",
    "2. al secondo step, l'algoritmo assegna ogni punto al centroide più vicino, definendo i  *cluster iniziali*;\n",
    "\n",
    "3. al terzo step, l'algoritmo *ricalcola il centroide* considerando il valore medio di tutti i punti del cluster, e ritorna allo step 2.\n",
    "\n",
    "Il k-means proseguirà fino a che i cluster calcolati al punto 2 non saranno stabili o, nei casi più complessi, fino a che non sarà raggiunto il numero massimo di iterazioni impostato in fase di inizializzazione. In figura possiamo osservare una spiegazione visiva del funzionamento dell'algoritmo.\n",
    "\n",
    "![kmeans](Images\\kmeans.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scelta del valore ottimale di cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La scelta del valore ottimale di $k$ è un procedimento emnpirico, in quanto non abbiamo a disposizione delle vere e proprie label per la verifica dell'uscita dell'algoritmo.\n",
    "\n",
    "In tal senso, abbiamo a disposizione sia delle metriche, che vedremo in seguito, sia degli approcci più qualitativi, che dipendono dai concetti di **cardinalità** e **magnitudine** del clustering.\n",
    "\n",
    "In particolare, per *cardinalità* si intende il *numero di campioni per ogni cluster*, mentre per *magnitudine* la *somma delle distanze di tutti i campioni in un cluster dal centroide*.\n",
    "\n",
    "Immaginiamo di essere in un caso come quello descritto nella seguente figura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cluster_eval](Images\\clustering_eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevedibilmente, il rapporto tra cardinalità e magnitudine dovrebbe essere all'incirca *lineare*. Quindi, come si può vedere dalla figura precedente, ci potrebbe essere qualcosa che non va con il cluster $4$.\n",
    "\n",
    "A questo punto, avendo valutato empiricamente la possibile presenza di un problema qualitativo con il clustering, possiamo provare ad eseguire l'algoritmo per un valore crescente di $k$.\n",
    "\n",
    "Proviamo a plottare questo valore in rapporto alla somma delle magnitudini del risultato, che diminuirà all'aumentare di $k$; un valore ottimale per $k$ è quello che si ottiene quando questo grafico tende a stabilizzarsi, ad esempio considerando il valore per cui la derivata diventa maggiore di -1 (e quindi l'angolo della funzione dei $k$ è maggiore di $135^°$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clust](Images\\clustering_k.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DBSCAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il **DBSCAN** è un algoritmo di clustering di tipo agglomerativo *density-based* che opera considerando due parametri principali:\n",
    "\n",
    "- la *distanza massima* $\\epsilon$ per considerare due punti come appartenenti allo stesso cluster;\n",
    "\n",
    "- il *numero minimo di campioni* $m$ per il quale è possibile definire un cluster.\n",
    "\n",
    "Nella pratica, il DBSCAN seleziona un campione casuale tra quelli non visitati, e valuta se ci sono $m$ campioni all'interno della distanza $\\epsilon$, nel qual caso si ha un *core point*.\n",
    "\n",
    "In alternativa, se il numero di campioni presenti in $\\epsilon$ è minore di $m$, ma comunque maggiore di 0, i campioni si dicono **$density reachable$** e, se connessi ad un *core point*, appartengono allo stesso cluster. \n",
    "\n",
    "Infine, se non vi sono campioni presenti in $\\epsilon$, allora il punto è isolato, ed è interpretato come un *outlier*.\n",
    "\n",
    "Un'interpretazione visiva è quella proposta in figura: in particolare, i punti in rosso definiscono diversi core points, i punti in giallo sono density reachable, e quindi fanno parte dello stesso cluster dei core points, mentre $N$ è un outlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dbscan](Images\\dbscan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Esercizi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.datasets import load_iris, make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Es 6.0**\n",
    "\n",
    "Il dataset Iris contiene i dati riguardanti lunghezza ed ampiezza di steli e petali per tre classi di fiori, ed è uno dei dataset \"standard\" per l'analisi dei dati nel machine learning. In tal senso, usiamo il metodo load_iris del package datasets di Scikit Learn per caricarlo. Una volta caricato in memoria, proviamo ad effettuare un primo clustering usando l'algoritmo k-means con 3 cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "iris.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Label veri')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "data = iris.data\n",
    "targets = iris.target\n",
    "X = data.loc[:, ('sepal length (cm)', 'sepal width (cm)')].values\n",
    "N_CLUSTERS = 3\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = sns.relplot(\n",
    "    data=data,\n",
    "    x='sepal length (cm)',\n",
    "    y='sepal width (cm)',\n",
    "    hue=targets)\n",
    "labels.fig.suptitle('Label veri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Label predetti dal clustering')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = kmeans.predict(X)\n",
    "predictions = sns.relplot(\n",
    "    data=data,\n",
    "    x='sepal length (cm)',\n",
    "    y='sepal width (cm)',\n",
    "    hue=y_pred)\n",
    "predictions.fig.suptitle('Label predetti dal clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Es 6.1**\n",
    "\n",
    "Verificare il valore di magnitudine e cardinalità per i cluster identificati nell'esercizio precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Cardinalities')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ottengo le distanze\n",
    "X_dist = kmeans.transform(X)\n",
    "\n",
    "# Funzione per calcolare magnitudine e cardinalità\n",
    "def get_magnitudes_cardinalities(dists):\n",
    "    clusters = np.argmin(dists, axis=1)\n",
    "    distances = np.amin(dists, axis=1)\n",
    "    dists_cens = list(zip(clusters, distances))\n",
    "    magnitudes = np.zeros(len(set(clusters)))\n",
    "    cardinalities = np.zeros(len(set(clusters)))\n",
    "    for dc in dists_cens:\n",
    "        magnitudes[dc[0]] += dc[1]\n",
    "        cardinalities[dc[0]] += 1\n",
    "    return magnitudes, cardinalities\n",
    "\n",
    "# Plotto magnitudine e cardinalità\n",
    "m, c = get_magnitudes_cardinalities(X_dist)\n",
    "cls_tags = list(range(N_CLUSTERS))\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.tight_layout(pad=3.0)\n",
    "ax1.bar(cls_tags, m)\n",
    "ax1.set_xticks([0, 1, 2])\n",
    "ax1.set_title('Magnitudes')\n",
    "ax2.bar(cls_tags, c)\n",
    "ax2.set_xticks([0, 1, 2])\n",
    "ax2.set_title('Cardinalities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2024b47f9e0e00785bf7b410773834da4047a250e58841a8c9925163fbfa3efc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
