{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduzione al Machine Learning**\n",
    "## *Introduzione, Definizione del Problema e Preparazione dei Dati*\n",
    "\n",
    "Introduzione al machine learning, classificazione dei sistemi di apprendimento, scelta del modello e preparazione dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduzione**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il **machine learning** è un sottoinsieme della *statistica*, ci offre il più delle volte un modo *più efficace* (ovviamente tale approccio non è in assoluto la soluzione migliore) di risolvere problemi complessi:\n",
    "\n",
    "- Prevede un processo di **addestramento** di un software, chiamato *modello*, che viene utilizzato per fare predizioni a partire da un insieme di dati\n",
    "- È contrapposto all’approccio tradizionale.\n",
    "\n",
    "Ad esempio:\n",
    "\n",
    "un sistema di previsioni meteo tradizionale si affida a complesse equazioni che stimano valori di umidità,\n",
    "pressione e pioggia, mentre un sistema di previsioni meteo basato su machine learning si affida ad un *algoritmo* in grado di fare previsioni a partire da un addestramento su grosse quantità di dati.\n",
    "\n",
    "Esistono sistemi ad apprendimento **supervisionato**, **non supervisionato** e di\n",
    "**reinforcement learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sistemi di apprendimento *Supervisionato***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sistemi ad apprendimento supervisionato partono da un insieme di **dati etichettati**. In pratica, un esperto di dominio definisce il valore da predire a partire per un certo campione, ed il modello è delegato ad individuare le relazioni che intercorrono tra le **feature** in input ed il valore atteso. La **label** può essere vista come la classe di appartenenza o il valore da dover raggiungere (*Processo di Labeling*).\n",
    "\n",
    "I sistemi ad apprendimento supervisionato si dividono in due categorie principali:\n",
    "- **Modelli di regressione** predicono un dato di tipo *numerico*\n",
    "- **Modelli di classificazione** predicono un dato di tipo *categorico* (ovvero una *classe*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sistemi di apprendimento *Non Supervisionato***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sistemi ad apprendimento non\n",
    "supervisionato inferiscono la\n",
    "suddivisione dei dati senza alcun\n",
    "apporto esterno. **Non necessitano di una etichettatura**\n",
    "effettuata da esperti di dominio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipici esempi sono:\n",
    "\n",
    "- **Algoritmi di clustering**: questi algoritmi suddividono lo spazio dei campioni sulla base di *metriche di distanza* o *criteri di agglomerazione*.\n",
    "\n",
    "Il clustering è diverso dalla classificazione! In questo caso non sappiamo quante labels ci sono, ossia quanti cluster ci sono a priori nello *spazio delle features* N-dimensionale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sistemi di *Reinforcement Learning***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sistemi di reinforcement learning\n",
    "sono concettualmente differenti da\n",
    "quelli di apprendimento:\n",
    "- Prevedono che un **agente** effettui una\n",
    "azione all’interno di un ambiente sulla\n",
    "base di una **ricompensa**\n",
    "- La ricompensa viene valutata sulla base\n",
    "dello stato attuale dell’ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definizione del Problema**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fondamentale nel machine learning come primo step è comprendere e capire l'**obiettivo** e l'**output atteso**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applicazione | Obiettivo del problema | Output atteso\n",
    ":-----: | :--------: | :-----:\n",
    "**Previsione Meteo**   | Calcolare le precipitazioni orarie in una determinata zona | Predizione delle precipitazione orarie\n",
    "**Spam Detector**  | Individuare lo spam  | Alert per un possibile spam\n",
    "**Previsione Bancaria**   | Identificare transazioni fraudolente | Blocco transazioni sospette\n",
    "**Identificazione degli oggetti**  | Individuare il tipo di oggetto in una foto  | Classe dell’oggetto rappresentato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati devono avere diverse *caratteristiche*:\n",
    "- Abbondanza (*In linea di massima ci serve almeno un ordine di grandezza in più rispetto al numero di parametri addestrabili*)\n",
    "- Consistenza\n",
    "- Affidabilità\n",
    "- Disponibilità\n",
    "- Correttezza\n",
    "- Rappresentatività\n",
    "\n",
    "Se i dati non le rispettano, potrebbe essere impossibile avere modelli con\n",
    "prestazioni adeguate.\n",
    "\n",
    "Ecco dei possibili esempi di dati ‘desiderabili’ nelle applicazioni precedenti:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applicazione | Dati desiderabili (un esempio)\n",
    ":-----: | :--------:\n",
    "**Previsione Meteo**   | Serie storiche su temperatura, pressione, umidità e precipitazioni per ogni giorno nei cento anni precedenti \n",
    "**Spam Detector**  | Numerosi esempi di mail ordinarie e di spam provenienti da mittenti eterogenei \n",
    "**Previsione Bancaria**   | Numerosi esempi di transazioni ordinarie e fraudolente prese in contesti eterogenei e variegati \n",
    "**Identificazione degli oggetti**  | Numerose foto di oggetti differenti in diverse angolazioni, pose, condizioni di illuminazione ed effetti di occlusione o rumore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E la successiva scelta del modello, strettament correlata agli step precedenti:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applicazione | Modello | Output atteso\n",
    ":-----: | :--------: | :-----:\n",
    "**Previsione Meteo**   | Modello di regressione univariata | Predizione delle precipitazione orarie\n",
    "**Spam Detector**  | Modello di classificazione binaria  | Alert per un possibile spam\n",
    "**Previsione Bancaria**   | Modello di classificazione binaria | Blocco transazioni sospette\n",
    "**Identificazione degli oggetti**  | Modello di classificazione multiclasse  | Classe dell’oggetto rappresentato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparazione dei Dati**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ottenere delle buone predizioni, è necessario costruire un dataset ed eventualmente effettuare delle opportune trasformazioni sui dati. Queste operazioni sono, salvo eccezioni, riassumibili nei concetti di **campionamento** e **preparazione** dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Campionamento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo problema da affrontare è la raccolta dei dati, che servirà ovviamente a generare il nostro dataset. In questa fase, dovremo partire affrontando due aspetti: la **dimensione** e la **qualità** dei dati che abbiamo raccolto.\n",
    "\n",
    "Non vi è una regola vera e propria per determinare il quantitativo di dati sufficiente per addestrare adeguatamente un modello. In generale, potremmo dire che il modello deve essere addestrato su un quantitativo di dati che sia *maggiore di almeno un ordine di grandezza* rispetto al numero dei parametri dello stesso. A scopo puramente esemplificativo, usando una rete neurale con  neuroni dovremmo indicativamente avere almeno  campioni a disposizione.\n",
    "\n",
    "Inoltre nell'ambito della data science, ciò implica che avere a disposizione grandi quantità di dati non basta se questi non sono anche **significativi** nella caratterizzazione del fenomeno sotto osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immaginiamo di voler creare un *modello di predizione delle precipitazioni* e di dover scegliere per addestrarlo tra due dataset. Il **dataset A** contiene i campionamenti ogni  dei valori di temperatura e di pressione degli ultimi , soltanto per il mese di luglio, mentre il **dataset B** contiene un unico valore giornaliero, ma preso per tutti i mesi dell'anno.\n",
    "\n",
    "Sebbene il **dataset A** possiede più dati, la qualità del **dataset B** è maggiore. Infatti il **dataset A** è inutile per la stima delle precipitazioni in inverno, primavera o autunno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dati | Quantità di dati | Qualità dei dati\n",
    ":-----: | :--------: | :-----:\n",
    "**A: acquisizioni meteo ogni 15 minuti per 100 anni, solo mese di luglio**   | $4 ⋅ 24 ⋅ 31 ⋅ 100 = 297.600$ | *Bassa*: modello in grado di prevedere solo precipitazioni luglio\n",
    "**B: acquisizioni meteo ogni giorno per 100 anni, tutti i mesi**  | $365 ⋅ 100 = 36.600$  | *Medio/alta*: modello in grado di prevedere precipitazioni per ogni mese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparazione dei dati**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completata la procedura di campionamento, è necessario effettuare la **preparazione** dei dati. \n",
    "\n",
    "In primis bisogna **rimuovere eventuali informazioni personali**. Il primo step è spesso trascurato, ma è di vitale importanza nel caso in cui si stia lavorando con delle informazioni sensibili, come informazioni legate alle condizioni sanitarie di diversi pazienti. In questi casi è strettamente necessario provvedere all'anonimizzazione dei dati, rimuovendo tutte le informazioni definite come Personally Identifiable Information (PII).\n",
    "\n",
    "Una volta completato questo passaggio, potremo passare alle azioni maggiormente rilevanti dal punto di vista scientifico:\n",
    "\n",
    "- **Errori nel labeling**: l’esperto di dominio ha svolto il suo compito in maniera ottimale?\n",
    "- **Rumorosità**: è importante valutare se i dati sono affetti da rumore. Ad esempio, le letture di un sensore        potrebbero essere tutte quante affette da offset o bias o, nel caso peggiore, essere causate da lettori non più tarati e quindi inutilizzabili.\n",
    "- **Dati mancanti**: è possibile che i valori di alcune feature non siano disponibili per alcuni campioni.\n",
    "- **Valori duplicati**: esistono dei campioni duplicati?\n",
    "- **Misure errate**: esistono delle misure errate o prese su range e scale differenti?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tutti questi casi, va scelta una strategia di pulizia: in certe situazioni potrebbe essere sufficiente eliminare un campione, oppure effettuare un'operazione di **filling** a partire dalla restante parte del dataset, o ancora, in casi estremi, si potrebbe eliminare completamente la feature interessata da rumore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sbilanciamento del dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' possibile che un dataset abbia diverse proporzioni nei **raggruppamenti** dei dati. Anche se questo fenomeno può interessare ogni insieme di dati, è maggiormente evidente nei problemi di classificazione, nei quali abbiamo un feedback immediato sulle differenti proporzioni grazie proprio alla presenza delle label per le classi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particolare avremmo due tipi di suddivisione:\n",
    "- **classi maggioritarie** ovvero quelle con il maggior numero di campioni.\n",
    "- **classi minoritarie** ovvero quelle con a disposizione un numero limitato di dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dataset in cui sussiste questa ineguaglianza è detto **sbilanciato**.\n",
    "\n",
    "E' possibile quantificare approssimativamente lo sbilanciamento del dataset. In tal senso, possiamo rifarci alla seguente tabella:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grado di Sbilanciamento | Percentuale di campioni di classi minoritarie\n",
    ":-----: | :--------:\n",
    "**Leggero**   | dal $20\\%$ al $40\\%$ del dataset  \n",
    "**Moderato**  | dal $1\\%$ al $20\\%$ del dataset  \n",
    "**Estremo**   | dal $<1\\%$ del dataset  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modo efficace per gestire situazioni in cui il dataset è sbilanciato è quello di utilizzare tecniche di **data balancing**. \n",
    "\n",
    "Ne esistono di diverse, più o meno efficaci. Tuttavia, la più semplice è quella di rimuovere un certo numero di campioni di classe maggioritaria (**sottocampionamento o downsampling**), dando agli esempi sottocampionati un peso maggiore nell'addestramento (**upweighting**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trasformazione dei dati**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo nella preparazione dei dati è quello di **trasformare** alcuni valori. In tal senso, possiamo operare per due ragioni principali.\n",
    "\n",
    "La prima è che siano necessarie delle trasformazioni obbligatorie volte a garantire la compatibilità dei dati, come ad esempio:\n",
    "\n",
    "- *convertire feature non numeriche in numeriche*: non è possibile effettuare operazioni sensate tra interi e stringhe, per cui bisogna individuare un modo per favorire il confronto.\n",
    "\n",
    "- *ridimensionare gli input ad una dimensione fissa*: alcuni modelli, come ad esempio le reti neurali, prevedono un numero fisso di nodi di input, per cui i dati in ingresso devono avere sempre la stessa dimensione.\n",
    "\n",
    "La seconda è legata invece a delle trasformazioni opzionali, che **ottimizzano** l'addestramento del modello. Ad esempio, potremmo dover effettuare la **normalizzazione** dei dati numerici, ovvero portarli tutti all'interno di una stessa scala di valori, normalmente compresa tra $[0,1]\\quad o \\quad [-1,1]$.\n",
    "\n",
    "Vediamo più nel dettaglio alcune possibilità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trasformazione dei dati numerici**\n",
    "\n",
    "Abbiamo detto in precedenza che potremmo voler applicare delle **normalizzazioni** a dei dati numerici per migliorare le performance del modello.\n",
    "\n",
    "Per comprenderne il motivo, immaginiamo di avere un dataset che comprende feature per età (che possiamo presupporre assuma valori da $0$ a $100$) e stipendio (che possiamo presupporre assuma valori da $10.000$€ a $100.000$€). Quando andiamo ad utilizzare questi valori in algoritmi che effettuano delle operazioni tra feature, l'età diventerà presto trascurabile rispetto allo stipendio, che è di due o tre ordini di grandezza superiore, per cui il modello si troverà a prediligere quest'ultimo in fase di analisi. Ciò implica quindi la necessità di arrivare ad una \"base comune\" a partire dalla quale operare.\n",
    "\n",
    "Le principali tecniche di normalizzazione disponibili sono quattro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scaling**\n",
    "\n",
    "     Prevede la conversione dei valori assunti da una feature in un range che va di solito tra $[0,1]\\quad o \\quad [-1,1]$. La formula dello scaling è la seguente:\n",
    "     $$y = \\frac{(x - x_{\\text{min}})}{(x_{\\text{max}} - x_{\\text{min}})}$$\n",
    "\n",
    "- **Clipping**:\n",
    "\n",
    "     Può capitare che il dataset contenga degli **outlier**, ovvero dei campioni che divergono notevolmente dalle caratteristiche statistiche del dataset. In questo caso, potremmo limitarci a rimuovere completamente tali valori mediante soglie statistiche, come i *range interquartili* in caso di distribuzione parametrica, o i classici $3\\sigma$ in caso di distribuzione normale.\n",
    "\n",
    "- **Trasformazione Logaritmica**\n",
    "\n",
    "     Un'altra possibilità è quella di convertire i nostri valori in scala logaritmica, comprimendo un range ampio in uno più piccolo usando la funzione logaritmo:\n",
    "     $$y = \\log{x}$$\n",
    "\n",
    "- **Z-Score**\n",
    "\n",
    "     Un ultimo tipo di trasformazione prevede l'uso dello _z-score_, che prevede una\n",
    "     riformulazione dei valori assunti dalla feature per fare in modo che questi\n",
    "     aderiscano ad una distribuzione a media nulla e deviazione standard unitaria.\n",
    "     Per calcolarlo, si usa la seguente formula:\n",
    "\n",
    "     $$y = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "     $\\text{dove } \\mu$ è la media della distribuzione dei nostri dati,\n",
    "     mentre $\\sigma$ è la varianza.     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trasformazione dei dati categorici**\n",
    "\n",
    "Alcune delle nostre feature possono assumere esclusivamente valori _discreti_.\n",
    "Ad esempio, le nostre immagini potrebbero raffigurare diverse razze di cani,\n",
    "oppure il campo \"località\" potrebbe riportare il codice postale. Queste feature\n",
    "sono conosciute come feature **categoriche**, ed i valori ad esse associate\n",
    "possono essere sia stringhe sia numeri.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per essere trattate, le feature categoriche hanno rappresentazioni di tipo **numerico**, mantenendo il riferimento al significato categorico e discreto. Per comprendere le implicazioni di questo concetto, immaginiamo i giorni della settimana. Il modo più semplice per passare da una rappresentazione puramente categorica ad una numerica è quella di usare un numero.\n",
    "In questa maniera creeremo un *dizionario*, nel quale potremo accedere ad una chiave (la rappresentazione) che rappresenterà un determinato valore (il giorno).\n",
    "\n",
    "Un altro modo di rappresentare le feature categoriche è mediante una **rappresentazione sparsa**, detta anche **one-hot encoding**, nella quale ogni valore è rappresentato da un vettore $V$ di lunghezza $m$, con $m$ numero di categorie possibili. In questo caso, tutti i valori di  saranno pari a $0$, tranne quello rappresentativo del valore attualmente assunto dalla feature, che sarà pari ad $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valore | Rappresentazione Intera | One-Hot Encoding\n",
    ":-----: | :--------: | :-----:\n",
    "**Lunedì**    | $1$ | $[1 0 0 0 0 0 0]$\n",
    "**Martedì**   | $2$ | $[0 1 0 0 0 0 0]$\n",
    "**Mercoledì** | $3$ | $[0 0 1 0 0 0 0]$\n",
    "**Giovedì**   | $4$ | $[0 0 0 1 0 0 0]$\n",
    "**Venerdì**   | $5$ | $[0 0 0 0 1 0 0]$\n",
    "**Sabato**    | $6$ | $[0 0 0 0 0 1 0]$\n",
    "**Domenica**  | $7$ | $[0 0 0 0 0 0 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Suddivisione dei dati**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ultimo passo nella preparazione del dataset è quello della **suddivisione dei dati**.\n",
    "\n",
    "In particolare, si destinano un certo quantitativo di dati per l'**addestramento del modello**, delegando la restante parte alla **validazione** dei risultati ottenuti: ciò è legato alla volontà di verificare la capacità di generalizzazione del modello, ovvero a quanto è in grado di \"funzionare\" il nostro algoritmo in caso di analisi di dati su cui non è stato addestrato.\n",
    "\n",
    "Un rapporto molto usato in tal senso è quello che prevede che il **$70\\%$** dei dati sia usato per l'addestramento, mentre il restante **$30\\%$** per la validazione dei risultati ottenuti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2024b47f9e0e00785bf7b410773834da4047a250e58841a8c9925163fbfa3efc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
