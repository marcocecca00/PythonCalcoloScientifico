{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Esercizi TensorFlow**\n",
    "## *Esercitazione TensorFlow e Keras*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Es 11.0**\n",
    "\n",
    "Scarichiamo il dataset *flowers*. Per farlo, usiamo il seguente codice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228813984/228813984 [==============================] - 29s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from tensorflow import keras\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = keras.utils.get_file(origin=dataset_url, extract=True)\n",
    "data_dir = pathlib.Path(data_dir).parent\n",
    "data_dir = pathlib.Path(data_dir, 'flower_photos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer (type) | Output Shape | Param # \n",
    ":-----: | :--------: | :-----:\n",
    "**rescaling_1 (Rescaling)**   |  (None, 150, 150, 3)  | 0\n",
    "**conv_1 (Conv2D)**  |(None, 148, 148, 32)  | 896\n",
    "**max_pool_1 (MaxPooling2D)**  |(None, 74, 74, 32)  | 0\n",
    "**conv_2 (Conv2D)**   |   (None, 72, 72, 32)   | 9248\n",
    "**max_pool_2 (MaxPooling2D)**  |  (None, 36, 36, 32)   | 0\n",
    "**dropout (Dropout)**  |   (None, 36, 36, 32)  | 0\n",
    "**flatten_1 (Flatten)**   |  (None, 41472)  | 0\n",
    "**classification (Dense)**  | (None, 5)    | 207365    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferiamo il *numero di classi* del dataset (ovvero la X nel precedente sommario) usando l'attributo `class_names` del dataset ottenuto da `image_dataset_from_directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from tensorflow import keras\n",
    "\n",
    "# Scarichiamo il dataset Flowers.\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = keras.utils.get_file(origin=dataset_url, extract=True)\n",
    "# Selezioniamo la cartella dove sono presenti i dati. Di solito è ~\\user\\username\\.keras\\dataset.\n",
    "data_dir = pathlib.Path(data_dir).parent\n",
    "data_dir = pathlib.Path(data_dir, 'flower_photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "# Creiamo i dataset di training e validazione. Usiamo un validation\n",
    "# split di 0.2 (quindi con rapporto 80/20).\n",
    "train = keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED)\n",
    "\n",
    "val = keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classi nel dataset: ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "# Mostriamo a schermo le classi presenti nel dataset usando\n",
    "# l'attributo class_names.\n",
    "print(f'Classi nel dataset: {train.class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "92/92 [==============================] - 25s 264ms/step - loss: 1.3072 - sparse_categorical_accuracy: 0.4251 - val_loss: 1.0861 - val_sparse_categorical_accuracy: 0.5627\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 26s 285ms/step - loss: 0.9858 - sparse_categorical_accuracy: 0.6151 - val_loss: 1.0254 - val_sparse_categorical_accuracy: 0.5926\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 27s 288ms/step - loss: 0.8284 - sparse_categorical_accuracy: 0.7033 - val_loss: 1.0012 - val_sparse_categorical_accuracy: 0.6104\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 28s 301ms/step - loss: 0.6542 - sparse_categorical_accuracy: 0.7735 - val_loss: 1.0452 - val_sparse_categorical_accuracy: 0.5940\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 30s 322ms/step - loss: 0.4895 - sparse_categorical_accuracy: 0.8341 - val_loss: 1.0266 - val_sparse_categorical_accuracy: 0.6376\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 27s 297ms/step - loss: 0.3490 - sparse_categorical_accuracy: 0.8896 - val_loss: 1.1973 - val_sparse_categorical_accuracy: 0.6322\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 29s 312ms/step - loss: 0.2740 - sparse_categorical_accuracy: 0.9128 - val_loss: 1.3077 - val_sparse_categorical_accuracy: 0.6090\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 26s 283ms/step - loss: 0.1860 - sparse_categorical_accuracy: 0.9431 - val_loss: 1.3387 - val_sparse_categorical_accuracy: 0.6008\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 26s 279ms/step - loss: 0.1629 - sparse_categorical_accuracy: 0.9561 - val_loss: 1.4735 - val_sparse_categorical_accuracy: 0.6172\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 26s 279ms/step - loss: 0.1377 - sparse_categorical_accuracy: 0.9591 - val_loss: 1.7358 - val_sparse_categorical_accuracy: 0.5599\n"
     ]
    }
   ],
   "source": [
    "# Creiamo ed addestriamo un modello.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(150, 150, 3)),\n",
    "        keras.layers.Rescaling(1./255),\n",
    "        keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation='relu',\n",
    "            name='conv_1'),\n",
    "        keras.layers.MaxPooling2D(name='max_pool_1'),\n",
    "        keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation='relu',\n",
    "            name='conv_2'),\n",
    "        keras.layers.MaxPooling2D(name='max_pool_2'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(\n",
    "            5,\n",
    "            name='classification',\n",
    "            activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Copiamo il modello (ci servirà per il passo successivo)\n",
    "model_callbacks = keras.models.clone_model(model)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "92/92 [==============================] - 29s 310ms/step - loss: 1.3601 - sparse_categorical_accuracy: 0.4172 - val_loss: 1.1183 - val_sparse_categorical_accuracy: 0.5109\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 28s 296ms/step - loss: 1.0638 - sparse_categorical_accuracy: 0.5848 - val_loss: 1.1016 - val_sparse_categorical_accuracy: 0.5354\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 27s 295ms/step - loss: 0.8942 - sparse_categorical_accuracy: 0.6683 - val_loss: 1.0759 - val_sparse_categorical_accuracy: 0.5845\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 28s 302ms/step - loss: 0.7047 - sparse_categorical_accuracy: 0.7476 - val_loss: 1.0143 - val_sparse_categorical_accuracy: 0.6022\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 27s 295ms/step - loss: 0.5479 - sparse_categorical_accuracy: 0.8127 - val_loss: 1.1474 - val_sparse_categorical_accuracy: 0.5817\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 27s 296ms/step - loss: 0.4504 - sparse_categorical_accuracy: 0.8532 - val_loss: 1.1543 - val_sparse_categorical_accuracy: 0.5872\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 27s 294ms/step - loss: 0.3408 - sparse_categorical_accuracy: 0.8873 - val_loss: 1.2133 - val_sparse_categorical_accuracy: 0.6131\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 30s 329ms/step - loss: 0.2489 - sparse_categorical_accuracy: 0.9186 - val_loss: 1.2801 - val_sparse_categorical_accuracy: 0.6172\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 31s 341ms/step - loss: 0.1980 - sparse_categorical_accuracy: 0.9360 - val_loss: 1.4159 - val_sparse_categorical_accuracy: 0.6090\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 32s 349ms/step - loss: 0.1853 - sparse_categorical_accuracy: 0.9458 - val_loss: 1.4253 - val_sparse_categorical_accuracy: 0.5763\n"
     ]
    }
   ],
   "source": [
    "# Riaddestriamo il modello utilizzando i callback.\n",
    "model_callbacks.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_sparse_categorical_accuracy',\n",
    "        min_delta=0.01,\n",
    "        patience=3,\n",
    "        restore_best_weights=True),\n",
    "    keras.callbacks.TensorBoard()\n",
    "]\n",
    "\n",
    "history = model_callbacks.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    callbacks=callbacks,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Es 11.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scarichiamo il dataset *Stack Overflow* e utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer (type) | Output Shape | Param # \n",
    ":-----: | :--------: | :-----:\n",
    "**dense_1 (Dense)**   |(None, 64) | 64064\n",
    "**dense_2 (Dense)**  |(None, 4)   | 260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usiamo gli stessi *callback* utilizzati in precedenza.\n",
    "\n",
    "Inferiamo il numero di classi del dataset (ovvero la X nel precedente sommario) usando l'attributo `class_names` del dataset ottenuto da `text_dataset_from_directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
      "6053168/6053168 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Scarichiamo e salviamo il dataset Stack Overflow.\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "data_dir = keras.utils.get_file(origin=dataset_url,\n",
    "                            extract=True,\n",
    "                            cache_subdir='datasets/stack_overflow')\n",
    "# Selezioniamo la cartella dove sono presenti i dati. \n",
    "# Di solito è ~\\user\\username\\.keras\\dataset.\n",
    "# Da notare che questo dataset ha al suo interno due cartelle,\n",
    "# una di train ed una di test. Nel nostro esercizio utilizzeremo\n",
    "# soltanto quella di train.\n",
    "data_dir = pathlib.Path(data_dir).parent\n",
    "train_dir = pathlib.Path(data_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "# Creiamo i datset di training e validazione. Usiamo un validation\n",
    "# split di 0.2 (quindi con rapporto 80/20).\n",
    "train = keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED)\n",
    "\n",
    "val = keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classi nel dataset: ['csharp', 'java', 'javascript', 'python']\n"
     ]
    }
   ],
   "source": [
    "# Mostriamo a schermo le classi presenti nel dataset usando\n",
    "# l'attributo class_names.\n",
    "print(f'Classi nel dataset: {train.class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un layer di tipo TextVectorization.\n",
    "VOCAB_SIZE = 1000\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')\n",
    "# Chiamiamo il metodo adapt sul testo da caratterizzare.\n",
    "train_text = train.map(lambda text, _: text)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 3s 11ms/step - loss: 0.9195 - sparse_categorical_accuracy: 0.6394 - val_loss: 0.6140 - val_sparse_categorical_accuracy: 0.7594\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5412 - sparse_categorical_accuracy: 0.7906 - val_loss: 0.5375 - val_sparse_categorical_accuracy: 0.7844\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4415 - sparse_categorical_accuracy: 0.8314 - val_loss: 0.5269 - val_sparse_categorical_accuracy: 0.7912\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3865 - sparse_categorical_accuracy: 0.8537 - val_loss: 0.5429 - val_sparse_categorical_accuracy: 0.7837\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3445 - sparse_categorical_accuracy: 0.8722 - val_loss: 0.5603 - val_sparse_categorical_accuracy: 0.7794\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3094 - sparse_categorical_accuracy: 0.8880 - val_loss: 0.5820 - val_sparse_categorical_accuracy: 0.7781\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.2754 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.5958 - val_sparse_categorical_accuracy: 0.7837\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.2460 - sparse_categorical_accuracy: 0.9173 - val_loss: 0.6200 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.2164 - sparse_categorical_accuracy: 0.9331 - val_loss: 0.6443 - val_sparse_categorical_accuracy: 0.7819\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1868 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6793 - val_sparse_categorical_accuracy: 0.7819\n"
     ]
    }
   ],
   "source": [
    "# Integriamo il layer di vettorizzazione all'interno di un\n",
    "# modello di tipo sequenziale.\n",
    "model = keras.models.Sequential()\n",
    "model.add(\n",
    "    keras.Input(shape=(1,),\n",
    "    dtype=tf.string))\n",
    "model.add(\n",
    "    vectorize_layer)\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        64,\n",
    "        activation='relu',\n",
    "        name='dense_1'))\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        4,\n",
    "        activation='softmax',\n",
    "        name='dense_2'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_sparse_categorical_accuracy',\n",
    "        min_delta=0.01,\n",
    "        patience=3,\n",
    "        restore_best_weights=True),\n",
    "    keras.callbacks.TensorBoard()\n",
    "]\n",
    "\n",
    "history=model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2024b47f9e0e00785bf7b410773834da4047a250e58841a8c9925163fbfa3efc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
