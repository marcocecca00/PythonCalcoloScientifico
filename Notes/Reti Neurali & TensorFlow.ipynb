{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reti Neurali & Introduzione a TensorFlow**\n",
    "## *Problemi non lineari, Reti Neurali e TensorFlow*\n",
    "\n",
    "Le reti neurali sono ormai sulla bocca di tutti: chiunque le usa per risolvere con successo ogni tipo di problema, ad esempio per la *classificazione* e *regressione*.\n",
    "\n",
    "Tuttavia, il passo giusto prima di usarle è quello di comprendere effettivamente a cosa servono: in tal senso, introduciamo il concetto di *problema non lineare*. Successivamente verrà fatta una breve introduzioen alla libreria *TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problemi non lineari**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diamo un'occhiata al dataset rappresentato nella figura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nonlin](Images\\nonlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo dataset è evidentemente *non lineare*: in pratica, questo significa che non esiste un algoritmo in grado di separare in maniera lineare le due classi tra loro o, in termini matematici, non esiste un modello in forma:\n",
    "\n",
    "$$y=ax_1+bx_2+c$$\n",
    "\n",
    "che permette di determinare $y$ a partire dalle feature $x_1$ e $x_2$. Ovviamente, se il numero di feature fosse più elevato, il sommatore lineare dovrebbe considerare un maggior numero di variabili indipendenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I lettori più audaci potrebbero provare ad usare delle approssimazioni lineari a tratti. Costoro dovrebbero considerare la seguente figura, estremamente non lineare:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](Images\\very_nonlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Reti Neurali & Problemi non Lineari**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per capire come le reti neurali ci aiutano a modellare un problema non lineare, visualizziamo un semplice sommatore pesato. Questo tipo di sommatore è in grado di modellare solo problemi lineari del tipo: $y=w_1x_1+w_2x_2+w_3x_3+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sommatorePesato](Images\\linear_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo semplice caso abbiamo tre input ed un output. Proviamo ad aggiungere un ulteriore strato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sommatore2](Images\\linear_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo strato che abbiamo aggiunto è detto **nascosto**, e rappresenta una serie di valori intermedi considerati dal sommatore nel calcolo dell'uscita.\n",
    "\n",
    "Quest'ultima non sarà più una somma pesata degli input, ma una *somma pesata dei valori in uscita dallo strato nascosto*, che a loro volta sono *dipendenti* dall'input.\n",
    "\n",
    "Tuttavia, il modello rimane *lineare*: potremo aggiungere un numero arbitrario di strati nascosti, ma questo sarà sempre vero, a meno che non si usi una particolare funzione, detta di **attivazione**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Funzione di Attivazione**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La modellazione di un problema non lineare prevede l'introduzione di (appunto) *non linearità* all'interno del modello. Nella pratica, potremo inserire delle opportune **funzioni non lineari** tra i diversi *strati* della rete. Queste funzioni sono dette di **attivazione**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attivazione](Images\\nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovviamente, con un maggior numero di strati nascosti, l'impatto delle non linearità diventa maggiore: in questo modo, saremo in grado di inferire delle relazioni anche molto complesse tra gli input e gli output (predetti).\n",
    "\n",
    "Le funzioni di attivazione più utilizzate in passato erano di tipo *sigmoidale* (simili, per intenderci, alla funzione che abbiamo visto in uscita alla regressione logistica).\n",
    "\n",
    "Attualmente, le funzioni più usate sono le **rectified linear unit**, o **ReLU**, che hanno risultati comparabili in termini di accuratezza del modello alla sigmoidale, ma risultano essere significativamente *meno complesse* dal punto di vista computazionale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le ReLU sono espresse dalla seguente funzione:\n",
    "\n",
    "$$y=max(0,x)$$\n",
    "\n",
    "che graficamente si traduce in una forma espressa come:\n",
    "\n",
    "![relu](Images\\relu.png)\n",
    "\n",
    "In pratica, una ReLU \"fa passare\" soltanto i valori positivi, portando a zero tutti quelli negativi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riassumendo:\n",
    "\n",
    "- una rete neurale è data da un **insieme di nodi**, o **neuroni**, organizzati in uno o più **strati**;\n",
    "\n",
    "- ogni neurone è connesso a quelli dello strato successivo mediante dei **pesi**, che rappresentano la \"forza\" della connessione;\n",
    "\n",
    "- esiste una **funzione di attivazione** che trasforma l'uscita di ogni neurone verso lo strato successivo inserendo delle non linearità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TensorFlow e Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow** è una libreria software *open source* per il *machine learning*, che fornisce moduli sperimentati e ottimizzati, utili nella realizzazione di algoritmi per diversi tipi di compiti percettivi e di comprensione del linguaggio.\n",
    "\n",
    "La sua interfaccia è abbastanza a basso livello, basato sull'utilizzo di *oggetti tensoriali*. Pertanto per facilitarne l'uso utilizzeremo una libreria basata su TensorFlow, `Keras`, successivamente integrata in esso.\n",
    "\n",
    "In particolare **keras** è una **API** (*interfaccia di programmazione di una applicazione*), progettata come un'interfaccia a un livello di *astrazione superiore* di altre librerie simili di più basso livello e supporta come *back-end* la libreria *TensorFlow*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import reuters, boston_housing\n",
    "from keras import models, layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad esempio affrontiamo un problema di **classficazione** usando il dataset built-in di keras *\"Reuters\"*, la cui descrizione è:\n",
    "\n",
    "*\" This is a dataset of 11,228 newswires from Reuters, labeled over 46 topics.*\n",
    "\n",
    "*Each newswire is encoded as a list of word indexes (integers).*\n",
    "\n",
    "*For convenience, words are indexed by overall frequency in the dataset,*\n",
    "*so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. \"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simile alla funzione di Scikit Learn \"train_test_split\" dove ci sono 4 valori restituiti\n",
    "# qui vengono restituite invece 2 tuple\n",
    "\n",
    "# Il valore delle x cambia (sono articoli di giornale quindi hanno lunghezze quindi parole diverse)\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing avanzato\n",
    "# Rendo omogeneo il dataset, lo 'tokenizzo'\n",
    "\n",
    "num_classes = np.amax(y_train)+1   # Garantisco coerenza così tra y_train e y_test\n",
    "tokenizer = Tokenizer(num_words=1000) # Numero massimo di parole che voglio mantenere (le parole sono ordinate per numero di occorrenze)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') # Converte x_train in array fissi \"1\" se c'è la parola tra le 1000(tokenizer) \"0\" se non c'è\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "y_train = to_categorical(y_train, num_classes) # Trasforma la rappresentazione di y_train\\test in una serie di 0 e 1. \"One-hot Encoder\"\n",
    "y_test = to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras** offre due interfacce per creare una *rete neurale*\n",
    "\n",
    "- **Interfaccia Sequenziale**: prevede dei livelli della rete neurali che siano aspirabili a delle *sequenze di strati nascosti* (input --> 1° strato nascosto --> 2° strato nascosto --> ... --> output)\n",
    "\n",
    "- **Interfaccia Funzionale**: alcune reti neurali hanno delle biforcazioni, possono muoversi tra i layers, sono strutture più complesse.\n",
    "\n",
    "Tra i tipi di *layers* esistono i **Dense Layer**, chiamati tali perchè è **completamente connesso** (come la figura di rete neurale vista precedentemente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 8008      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 46)                414       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,422\n",
      "Trainable params: 8,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Modello con 1 un layer con 8 neuroni\n",
    "# L'input è agganciato alla x e l'output alla y\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_shape=(1000,))) \n",
    "model.add(layers.Dense(num_classes, activation='softmax')) #Softmax è simile alla funzione di regressione logistica\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver creato il nostro modello, dobbiamo specificare:\n",
    "\n",
    "1. **L'ottimizzatore** ci permette di ottimizzare la funzione di costo (*\"SVG\", \"Adam\", ...*)\n",
    "\n",
    "2. **La funzione di costo** ad esempio *categorical crossentropy, sparse crossentropy e binary crossentropy\"*. La binary per classificaizone binarie, categorical per le categoriche\n",
    "\n",
    "3. **La metrica** come ad esempio la *accuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "281/281 [==============================] - 3s 4ms/step - loss: 2.3836 - acc: 0.4978\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 1s 4ms/step - loss: 1.4014 - acc: 0.6935\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 1.1966 - acc: 0.7324\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 1.0819 - acc: 0.7538\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.9996 - acc: 0.7672\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.9342 - acc: 0.7808\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.8813 - acc: 0.7928\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 2s 5ms/step - loss: 0.8353 - acc: 0.8036\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.7973 - acc: 0.8104\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 2s 7ms/step - loss: 0.7609 - acc: 0.8167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15ba0cbb4f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Effettuo l'addestramento\n",
    "# Il batch è la quantità di dati che passo ad ogni epoca,\n",
    "# in quanto su ogni epoca lavoro su un subset dei dati\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservo che alla decima iterazione ho un determinato valore di *accuracy*. L'accuracy però si dovrebbe utilizzare sui *dati di test*.\n",
    "\n",
    "Usandola sui *dati di training* non riesco ad occergermi se ci è stato un *overfitting*, ossia se la rete si è affezionata ai dei meccanismi propri del dati di *training*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifico adesso il modello aggiungedo dei layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 16)                16016     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 46)                782       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,070\n",
      "Trainable params: 17,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Modello con 2 layers con 16 neuroni\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(1000,)))\n",
    "model.add(layers.Dense(16, activation='relu')) \n",
    "model.add(layers.Dense(num_classes, activation='softmax')) #Softmax è simile alla funzione di regressione logistica\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "281/281 [==============================] - 1s 4ms/step - loss: 2.1156 - acc: 0.5237\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 1.2529 - acc: 0.7098\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 1.0419 - acc: 0.7532\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.9002 - acc: 0.7864\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.7891 - acc: 0.8132\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.7065 - acc: 0.8366\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.6379 - acc: 0.8485\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.5803 - acc: 0.8625\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.5360 - acc: 0.8717\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 1s 5ms/step - loss: 0.4972 - acc: 0.8786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15b9edfa640>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Effettuo l'addestramento\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendenzialmente aumentare il *numero di neuroni* e *numero di epoche* **aumenta** l'efficienza della rete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affrontiamo adesso un problema di **regressione**, usando il dataset *\"Boston Housing\"*:\n",
    "\n",
    "*\"Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. Targets are the median values of the houses at a location (in k$)\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 6483.2173 - mae: 75.8719\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 941.5818 - mae: 27.6029\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 159.8709 - mae: 10.0551\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 102.2050 - mae: 7.4685\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 88.3844 - mae: 6.9702\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 78.4888 - mae: 6.5493\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 69.6548 - mae: 6.1739\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 66.2263 - mae: 5.9497\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 61.2764 - mae: 5.5986\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 63.0486 - mae: 5.8550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15ba01880a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modello a 32 neuroni addestrato per 10 epoche\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(13,)))\n",
    "model.add(layers.Dense(1)) #Nell'uscita è solo un valore predetto 1\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss ='mse', #mean squared error\n",
    "              metrics=['mae']) #mean absolute error\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2024b47f9e0e00785bf7b410773834da4047a250e58841a8c9925163fbfa3efc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
