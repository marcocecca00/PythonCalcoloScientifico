{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TensorFlow - Tips & Tricks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati che abbiamo utilizzato finora erano organizzati sotto forma di array NumPy. Tuttavia, per dataset di grosse dimensioni, potrebbe essere necessario utilizzare degli oggetti di tipo `Dataset`. In tal senso, Keras ci mette a disposizione diverse tecniche; vediamone alcuni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Immagini**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per caricare un dataset di immagini a partire da una cartella, possiamo usare la funzione `image_dataset_from_directory`, che ha una sintassi di questo tipo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=seed)\n",
    "\n",
    "val = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel precedente esempio:\n",
    "\n",
    "- `data_dir` è la cartella dove sono presenti i dati;\n",
    "- `validation_split` indica quanti dati usare per la validazione. Il valore deve essere coerente tra il dataset di train e quello di validazione;\n",
    "- `subset` indica se il dataset è indirizzato al training o alla validazione;\n",
    "- `image_size` rappresenta la dimensione (in pixel) dell'immagine;\n",
    "- `batch_size` rappresenta la dimensione del batch di dati da usare;\n",
    "- `seed` è un parametro che ci assicura la coerenza tra le immagini scelte per il training e quelle scelte per il test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cartella data_dir deve essere organizzata come segue:\n",
    "\n",
    "data_dir/\n",
    "\n",
    "...class1/\n",
    "\n",
    "......1.png\n",
    "\n",
    "......2.png\n",
    "\n",
    "...class2/\n",
    "\n",
    "......1.png\n",
    "\n",
    "......2.png\n",
    "\n",
    "......3.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo passare `train` e `val` direttamente al metodo fit del nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un accorgimento utile a migliorare le prestazioni della nostra rete è quello di inserire un **layer di rescaling** qualora si abbia a che fare con **immagini a colori**. Infatti, i canali RGB possono assumere valori all'interno del range $[0,255]$, mentre è consigliabile usare per una rete neurale valori compresi nell'intervallo $[0,1]$. Keras ci mette a disposizione un apposito layer:\n",
    "\n",
    "py `tf.keras.layers.Rescaling(1./255)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras offre un metodo simile per creare un dataset a partire da un insieme di **file di testo**, utilizzando il metodo `text_dataset_from_directory`.\n",
    "\n",
    "Analogamente al metodo usato per le immagini, `text_dataset_from_directory` si aspetta una cartella in una certa forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_dir/\n",
    "\n",
    "...class1/\n",
    "\n",
    "......1.txt\n",
    "\n",
    "......2.txt\n",
    "\n",
    "...class2/\n",
    "\n",
    "......1.txt\n",
    "\n",
    "......2.txt\n",
    "\n",
    "......3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per caricare il nostro dataset possiamo usare una forma analoga a quella delle immagini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_dataset_from_direcotry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp;\\ipykernel_21016\\2929199106.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train = text_dataset_from_direcotry(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_dataset_from_direcotry' is not defined"
     ]
    }
   ],
   "source": [
    "train = text_dataset_from_direcotry(\n",
    "    data_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "val = text_dataset_from_directory(\n",
    "    data_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I parametri sono esattamente gli stessi, a meno dell'assenza del parametro `image_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparazione dei dati testuali per il training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rispetto alle immagini, i dati testuali richiedono *tre* ulteriori operazioni, ovvero:\n",
    "\n",
    "- **standardizzazione**: si tratta di una procedura di preprocessing sul testo, che consiste tipicamente nella *rimozione della punteggiatura*. Di default, questa operazione converte l'intero testo in minuscolo e rimuove la punteggiatura;\n",
    "\n",
    "- **tokenizzazione**: si tratta di una procedura di suddivisione delle stringhe in *token*. Ad esempio, si può suddividere una frase nelle singole parole. Di default, questa operazione suddivide i token in base allo spazio;\n",
    "\n",
    "- **vettorizzazione**: si tratta della procedura di conversione dei token in valori numerici trattabili da un modello di rete neurale. Di default, il metodo di vettorizzazione è `int`.\n",
    "\n",
    "Questi tre step sono gestiti in automatico da un layer chiamato `TextVectorization`.\n",
    "\n",
    "Procediamo quindi a creare un layer di TextVectorization utilizzando una vettorizzazione *binaria*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode ='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particolare, `max_tokens` permette di stabilire il numero massimo di vocaboli consentiti, mentre l'`output_mode` indica la modalità con cui sarà gestita la sequenza vettorizzat.\n",
    "\n",
    "A questo punto, occorre creare il dataset che sarà effettivamente passato al primo layer della rete neurale. In tal senso, dobbiamo tenere conto che i dataset che abbiamo creato mediante `text_dataset_from_directory` non sono ancora stati *vettorizzati*, ed inoltre le singole coppie **campione/label** non sono accessibili mediante le tecniche standard di indicizzazione. Ciò è legato al fatto che le funzioni `*_dataset_from_directory` creano un oggetto di tipo `BatchDataset`, usato da TensorFlow per *ottimizzare* il caricamento in memoria di dataset di grosse dimensioni.\n",
    "\n",
    "Di conseguenza, dovremo innanzitutto estrarre il testo *senza considerare* le singole label. Per farlo, possiamo usare la funzione `map()` del nostro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train.map(lambda text, labels: text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `map()` non fa altro che applicare all'intero iterabile la funzione passata come argomento. In tal senso, il parametro passato altro non è se non una **lambda function**, ovvero una funzione anonima che assume una forma sintattica del tipo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda args : expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e quindi applica l'espressione a valle dei `:` agli argomenti passati. In questo caso, stiamo semplicemente facendo in modo che tutte le coppie testo/label siano \"mappate\" sul semplice testo.\n",
    "\n",
    "Una volta estratto il testo, dovremo chiamare il metodo `adapt` del layer di vettorizzazione in modo tale da creare il vocabolario che associ un determinato token numerico a ciascuna stringa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potremo quindi procedere ad integrare il layer di vettorizzazione all'interno del nostro modello.\n",
    "\n",
    "In tal senso, dovremo assicurarci che il modello abbia un input di forma (1,) e tipo stringa, facendo in modo che la rete abbia un'unica stringa in input per ciascun batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "    keras.Input(shape=(1,),\n",
    "    dtype=tf.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso di un array NumPy, occorre utilizzare il metodo `from_tensor_slices`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = from_tensor_slices((x_train, y_train))\n",
    "val = from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Callback**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *callback* è un'azione che il modello può effettuare durante diverse fasi dell'apprendimento.\n",
    "\n",
    "Keras ne offre di numerosi, che possono essere utilizzati ad esempio per *monitorare le metriche* che abbiamo scelto per la valutazione del modello, o *salvare* lo stesso su disco.\n",
    "\n",
    "Per usare i callback, dovremo crearne una *lista*, e passarla al parametro `callbacks` usato dal metodo `fit()` del nostro modello.\n",
    "\n",
    "Proviamo, ad esempio, a creare un insieme di callback che permetta di salvare i pesi del modello con una certa frequenza, e che termini il training se lo stesso sta andando in overfitting.\n",
    "\n",
    "Per prima cosa, creiamo un oggetto di tipo *`ModelCheckpoint`*, che ci permette di salvare i pesi del modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=path_to_checkpoints,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particolare:\n",
    "\n",
    "- `filepath` indica il percorso del file nel quale salveremo i checkpoint;\n",
    "\n",
    "- `save_weights_only` istruisce Keras a salvare soltanto i pesi del modello, riducendo lo spazio occupato in memoria;\n",
    "monitor indica la metrica da monitorare;\n",
    "\n",
    "- `save_best_only` istruisce Keras a salvare soltanto il modello \"migliore\", trascurando quelli ottenuti durante le altre iterazioni.\n",
    "\n",
    "Proviamo poi a creare un oggetto di tipo EarlyStopping, che ci permette di terminare l'addestramento qualora la metrica monitorata non presenti miglioramenti tra un'epoca e l'altra. Ad esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.1,\n",
    "    patience=3,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel codice precedente:\n",
    "\n",
    "- `monitor` indica la metrica da monitorare;\n",
    "\n",
    "- `min_delta` indica il valore minimo da considerare migliorativo per la metrica;\n",
    "\n",
    "- `patience` indica il numero di epoche dopo il quale il training viene interrotto in assenza di miglioramenti;\n",
    "\n",
    "- `restore_best_weights` indica se ripristinare i valori migliori ottenuti per i parametri dopo il termine dell'addestramento, o se usare gli ultimi.\n",
    "\n",
    "Aggiungiamo infine un ultimo callback, da utilizzare per permettere di *visualizzare* i risultati del nostro training su un tool di visualizzazione chiamato `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorBoard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp;\\ipykernel_21016\\2528884990.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtb_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'TensorBoard' is not defined"
     ]
    }
   ],
   "source": [
    "tb_callback = TensorBoard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per TensorBoard, possiamo lasciare i parametri al loro valore di default. La reference completa è disponibile sulla [documentazione ufficiale](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard).\n",
    "\n",
    "Possiamo adesso specificare i callback da utilizzare passando le precedenti variabili sotto forma di lista al metodo `fit()` del nostro modello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transfer learning e fine tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reti neurali, e soprattutto le CNN, presentano un'interessante caratteristica, ovvero quella di **apprendere delle feature** di carattere generico nei loro primi strati, andandosi a **specializzare** man mano che si va in profondità nella rete.\n",
    "\n",
    "Partendo da questa considerazione è stata elaborata la tecnica del **transfer learning**, che consiste nel prendere il modello addestrato su un problema e riconfigurarlo per risolverne uno *nuovo* (ma, ovviamente, simile: ad esempio, un tool che permette di valutare la razza di un gatto può essere usato per distinguere tra leopardi e tigri). Questa tecnica permette anche di addestrare un numero *limitato di parametri*, per cui è possibile utilizzarla quando si ha a che fare con dataset di dimensioni limitate.\n",
    "\n",
    "In tal senso, il transfer learning segue di solito questi step:\n",
    "\n",
    "- consideriamo i layer ed i pesi di un modello *addestrato in precedenza*;\n",
    "\n",
    "- effettuiamo il *freezing* (congelamento) di questi layer, fissando i valori dei pesi;\n",
    "\n",
    "- creiamo ed inseriamo alcuni layer successivi a quelli congelati per adattarli al nostro problema;\n",
    "\n",
    "- addestriamo i nuovi layer sul nostro problema.\n",
    "\n",
    "Opzionalmente, è possibile effettuare un passaggio di **fine tuning**, \"*sbloccando*\" il modello ottenuto in precedenza e riaddestrandolo sull'intero dataset con un basso *learning rate*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2024b47f9e0e00785bf7b410773834da4047a250e58841a8c9925163fbfa3efc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
